{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michalnt/miniconda3/envs/typical_deeplearning/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "# from DiagnosisAI.models.AutoEncoder_BRAIN import Segmenter\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bw_RGB(image):\n",
    "    temp_img = image.astype(np.float32)\n",
    "    temp_img *= 1 / temp_img.max() \n",
    "    temp_img = temp_img[np.newaxis, :, :]\n",
    "    # temp_img = cv.cvtColor(temp_img, cv.COLOR_GRAY2RGB)\n",
    "    temp_img = t.tensor(temp_img)\n",
    "    # temp_img = temp_img.permute(2, 0, 1)\n",
    "\n",
    "    return temp_img\n",
    "\n",
    "def conv_mask(mask):\n",
    "    return mask[np.newaxis, :, :].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"../datasets/brain/Brats2021_training_df/BraTS2021_00000/BraTS2021_00000_flair.nii.gz\")\n",
    "seg_path = Path(\"../datasets/brain/Brats2021_training_df/BraTS2021_00000/BraTS2021_00000_seg.nii.gz\")\n",
    "file_test = nib.load(file_path).get_fdata()\n",
    "label_test = nib.load(seg_path).get_fdata()\n",
    "\n",
    "imgs_train = [convert_bw_RGB(file_test[:, :, 80]) for _ in range(50)]\n",
    "labels_train = [conv_mask(label_test[:, :, 80]) for _ in range(50)]\n",
    "\n",
    "imgs_val = [convert_bw_RGB(file_test[:, :, 80]) for _ in range(10)]\n",
    "labels_val = [conv_mask(label_test[:, :, 80]) for _ in range(10)]\n",
    "\n",
    "imgs_test = [convert_bw_RGB(file_test[:, :, 80]) for _ in range(15)]\n",
    "labels_test = [conv_mask(label_test[:, :, 80]) for _ in range(15)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = t.utils.data.DataLoader(list(zip(imgs_train, labels_train)), batch_size=1, num_workers=8)\n",
    "val_loader = t.utils.data.DataLoader(list(zip(imgs_val, labels_val)), batch_size=1, num_workers=8)\n",
    "test_loader = t.utils.data.DataLoader(list(zip(imgs_test, label_test)), batch_size=1, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================== test above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. Unet domslnie wciaga 3 kanalowy obraz RGB, my mamy 1 kanal, ktory sie nie pokazuje wiec czy mozna sztucznie dodac os lub wczytac\n",
    "# wczytac jako rgb czyli dac to samo na 3 kanaly\n",
    "# 2. maska do segmentacji czy ma byc binarna dla kanalow tyle ile jest klas czy moze byc w jednym zdjeciu dla roznych wartosci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 240, 240]), torch.Size([1, 1, 240, 240]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, in_channels, nfilter, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    # input = batch_size, 1, 240, 240\n",
    "    x = nn.Conv2d(in_channels=in_channels, out_channels=nfilter, kernel_size=kernel_size, padding='same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = nn.BatchNorm2d(num_features=nfilter)(x)\n",
    "    x = nn.ReLU()(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = nn.Conv2d(in_channels=nfilter, out_channels=nfilter, kernel_size=kernel_size, padding='same')(x)\n",
    "    if batchnorm:\n",
    "        x = nn.BatchNorm2d(num_features=nfilter)(x)\n",
    "    x = nn.ReLU()(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = img\n",
    "in_channels = 1\n",
    "n_filters = 16\n",
    "batchnorm = True\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = conv2d_block(input_img, in_channels, n_filters * 1, kernel_size = 3, batchnorm = batchnorm) # (1, 16, 240, 240)\n",
    "p1 = nn.MaxPool2d((2, 2))(c1) # (1, 16, 120, 120)\n",
    "p1 = nn.Dropout(dropout)(p1)\n",
    "\n",
    "c2 = conv2d_block(p1, n_filters * 1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm) # (1, 32, 120, 120)\n",
    "p2 = nn.MaxPool2d((2, 2))(c2) # (1, 32, 60, 60)\n",
    "p2 = nn.Dropout(dropout)(p2)\n",
    "\n",
    "c3 = conv2d_block(p2, n_filters * 2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm) # (1, 64, 60, 60)\n",
    "p3 = nn.MaxPool2d((2, 2))(c3) # (1, 64, 30, 30)\n",
    "p3 = nn.Dropout(dropout)(p3)\n",
    "\n",
    "c4 = conv2d_block(p3, n_filters * 4, n_filters * 8, kernel_size = 3, batchnorm = batchnorm) # (1, 128, 30, 30)\n",
    "p4 = nn.MaxPool2d((2, 2))(c4) # (1, 128, 15, 15)\n",
    "p4 = nn.Dropout(dropout)(p4)\n",
    "\n",
    "c5 = conv2d_block(p4, n_filters * 8, n_filters * 16, kernel_size = 3, batchnorm = batchnorm) # (1, 256, 15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 15, 15])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "u6 = nn.ConvTranspose2d(n_filters * 16, n_filters * 8, kernel_size=3, padding=1)(c5) # (1, 128, 15, 15)\n",
    "u6 = nn.UpsamplingNearest2d(scale_factor=2)(u6) #blinear # (1, 128, 30, 30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get better precise locations, at every step of the decoder we use skip connections by concatenating the output of the transposed convolution layers with the feature maps from the Encoder at the same level:\n",
    "u6 = u6 + c4\n",
    "u7 = u7 + c3\n",
    "u8 = u8 + c2\n",
    "u9 = u9 + c1\n",
    "After every concatenation we again apply two consecutive regular convolutions so that the model can learn to assemble a more precise output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 128, 3, 3], expected input[1, 256, 30, 30] to have 128 channels, but got 256 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/michalnt/Desktop/repos/mgr/examples/brain_slices_seg.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michalnt/Desktop/repos/mgr/examples/brain_slices_seg.ipynb#ch0000016?line=2'>3</a>\u001b[0m u6 \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mcat([u6, c4], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# (1, 128, 30, 30) i (1, 128, 30, 30)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michalnt/Desktop/repos/mgr/examples/brain_slices_seg.ipynb#ch0000016?line=3'>4</a>\u001b[0m u6 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(dropout)(u6)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/michalnt/Desktop/repos/mgr/examples/brain_slices_seg.ipynb#ch0000016?line=4'>5</a>\u001b[0m c6 \u001b[39m=\u001b[39m conv2d_block(u6, n_filters \u001b[39m*\u001b[39;49m \u001b[39m8\u001b[39;49m, n_filters \u001b[39m*\u001b[39;49m \u001b[39m8\u001b[39;49m, kernel_size \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m, batchnorm \u001b[39m=\u001b[39;49m batchnorm) \u001b[39m# (1, 128, 30, 30)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michalnt/Desktop/repos/mgr/examples/brain_slices_seg.ipynb#ch0000016?line=6'>7</a>\u001b[0m u7 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConvTranspose2d(n_filters \u001b[39m*\u001b[39m \u001b[39m8\u001b[39m, n_filters \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, padding \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)(c6) \u001b[39m# (1, 64, 30, 30)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michalnt/Desktop/repos/mgr/examples/brain_slices_seg.ipynb#ch0000016?line=7'>8</a>\u001b[0m u7 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mUpsamplingNearest2d(scale_factor\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)(u7) \u001b[39m# (1, 64, 60, 60)\u001b[39;00m\n",
      "\u001b[1;32m/Users/michalnt/Desktop/repos/mgr/examples/brain_slices_seg.ipynb Cell 10'\u001b[0m in \u001b[0;36mconv2d_block\u001b[0;34m(input_tensor, in_channels, nfilter, kernel_size, batchnorm)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michalnt/Desktop/repos/mgr/examples/brain_slices_seg.ipynb#ch0000009?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michalnt/Desktop/repos/mgr/examples/brain_slices_seg.ipynb#ch0000009?line=2'>3</a>\u001b[0m \u001b[39m# first layer\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michalnt/Desktop/repos/mgr/examples/brain_slices_seg.ipynb#ch0000009?line=3'>4</a>\u001b[0m \u001b[39m# input = batch_size, 1, 240, 240\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/michalnt/Desktop/repos/mgr/examples/brain_slices_seg.ipynb#ch0000009?line=4'>5</a>\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mConv2d(in_channels\u001b[39m=\u001b[39;49min_channels, out_channels\u001b[39m=\u001b[39;49mnfilter, kernel_size\u001b[39m=\u001b[39;49mkernel_size, padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msame\u001b[39;49m\u001b[39m'\u001b[39;49m)(input_tensor)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michalnt/Desktop/repos/mgr/examples/brain_slices_seg.ipynb#ch0000009?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m batchnorm:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/michalnt/Desktop/repos/mgr/examples/brain_slices_seg.ipynb#ch0000009?line=6'>7</a>\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBatchNorm2d(num_features\u001b[39m=\u001b[39mnfilter)(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/typical_deeplearning/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/typical_deeplearning/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/typical_deeplearning/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 128, 3, 3], expected input[1, 256, 30, 30] to have 128 channels, but got 256 channels instead"
     ]
    }
   ],
   "source": [
    "u6 = nn.ConvTranspose2d(n_filters * 16, n_filters * 8, kernel_size=3, padding=1)(c5) # (1, 128, 15, 15)\n",
    "u6 = nn.UpsamplingNearest2d(scale_factor=2)(u6) #blinear # (1, 128, 30, 30) \n",
    "u6 = t.cat([u6, c4], dim=1) # (1, 128, 30, 30) i (1, 128, 30, 30) = (1, 256, 30, 30)\n",
    "u6 = nn.Dropout(dropout)(u6)\n",
    "c6 = conv2d_block(u6, n_filters * 16, n_filters * 8, kernel_size = 3, batchnorm = batchnorm) # (1, 128, 30, 30)\n",
    "\n",
    "u7 = nn.ConvTranspose2d(n_filters * 8, n_filters * 4, kernel_size=3, padding = 1)(c6) # (1, 64, 30, 30)\n",
    "u7 = nn.UpsamplingNearest2d(scale_factor=2)(u7) # (1, 64, 60, 60)\n",
    "u7 = t.cat([u7, c3], dim=1) # (1, 64, 60, 60) i (1, 64, 60, 60) = (1, 128, 60, 60)\n",
    "u7 = nn.Dropout(dropout)(u7)\n",
    "c7 = conv2d_block(u7, n_filters * 8, n_filters * 4, kernel_size = 3, batchnorm = batchnorm) # (1, 64, 60, 60)\n",
    "\n",
    "u8 = nn.ConvTranspose2d(n_filters * 4, n_filters * 2, kernel_size= 3, padding = 1)(c7) # (1, 32, 60, 60)\n",
    "u8 = nn.UpsamplingNearest2d(scale_factor=2)(u8) # (1, 32, 120, 120)\n",
    "u8 = t.cat([u8, c2], dim=1) # (1, 32, 120, 120) i (1, 32, 120, 120) = (1, 64, 120, 120)\n",
    "u8 = nn.Dropout(dropout)(u8)\n",
    "c8 = conv2d_block(u8, n_filters * 4, n_filters * 2, kernel_size = 3, batchnorm = batchnorm) # (1, 32, 120, 120)\n",
    "\n",
    "u9 = nn.ConvTranspose2d(n_filters * 2, n_filters * 1, kernel_size= 3, padding = 1)(c8) # (1, 16, 120, 120)\n",
    "u9 = nn.UpsamplingNearest2d(scale_factor=2)(u9) # (1, 16, 240, 240)\n",
    "u9 = t.cat([u9, c1], dim=1) # (1, 16, 240, 240) I (1, 16, 240, 240) = (1, 32, 240, 240)\n",
    "u9 = nn.Dropout(dropout)(u9)\n",
    "\n",
    "c9 = conv2d_block(u9, n_filters * 2, n_filters * 1, kernel_size = 3, batchnorm = batchnorm) # (1, 16, 240, 240)\n",
    "outputs = nn.Conv2d(n_filters * 1, in_channels, kernel_size=1)(c9)\n",
    "outputs = nn.Sigmoid()(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 240, 240])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value 1 - core\n",
    "# value 2 - invaded\n",
    "# value 4 - enhanced"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('typical_deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e576ea8c3537398da9dcdce81ba2027277556fb4790a3db5b00990a14b94fe39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
