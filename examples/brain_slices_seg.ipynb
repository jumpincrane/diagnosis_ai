{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "from DiagnosisAI.models.AutoEncoder_UNET.AutoEncoder_BRAIN_UNET import UNet\n",
    "import torchmetrics\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "# import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainSlicesDataset(t.utils.data.Dataset):\n",
    "    def __init__(self, file_names: list, transform: bool = True, binary_mask: bool = True):\n",
    "        self.root_path = Path(\"../datasets/brain/train_images_max_area/\")\n",
    "        self.file_names = file_names\n",
    "        self.image_size = (240, 240)\n",
    "        self.transform = transform\n",
    "        self.binary_mask = binary_mask\n",
    "        self.transforms = Compose([ToTensor()])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slice_pickle_path = self.root_path / self.file_names[idx]\n",
    "        with open(slice_pickle_path, 'rb') as handle:\n",
    "            slice_data = pickle.load(handle)\n",
    "        img = slice_data['flair'].astype(np.float32)\n",
    "        label = slice_data['seg']\n",
    "        # convert to binary mask\n",
    "        \n",
    "        if self.binary_mask:\n",
    "            label = np.where(label >= 1, 1, 0)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transforms(img)\n",
    "            label = self.transforms(label)\n",
    "            if self.binary_mask:\n",
    "                label = label.type(t.float32)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_pickle = []\n",
    "for dir in Path(\"../datasets/brain/train_images_max_area/\").iterdir():\n",
    "    for filename in dir.iterdir():\n",
    "        rel_path = Path(*filename.parts[-2:])\n",
    "        filenames_pickle.append(rel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names, val_names = train_test_split(filenames_pickle, test_size=0.2, random_state=42)\n",
    "val_names, test_names = train_test_split(val_names, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BrainSlicesDataset(train_names)\n",
    "val_dataset = BrainSlicesDataset(val_names)\n",
    "test_dataset = BrainSlicesDataset(test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = t.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4)\n",
    "val_loader = t.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n",
    "test_loader = t.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = UNet(in_channels=1, out_channels=1, n_filters=16)\n",
    "        # self.loss_function = nn.CrossEntropyLoss() for semantic\n",
    "        self.loss_function = nn.BCEWithLogitsLoss() # for binary \n",
    "        metrics = torchmetrics.MetricCollection([torchmetrics.Precision(num_classes=1, average='macro', mdmc_average='samplewise'),\n",
    "                                                 torchmetrics.Recall(num_classes=1, average='macro', mdmc_average='samplewise'),\n",
    "                                                 torchmetrics.F1Score(num_classes=1, average='macro', mdmc_average='samplewise'),\n",
    "                                                 torchmetrics.Accuracy(num_classes=1, average='macro', mdmc_average='samplewise')\n",
    "                                                ])\n",
    "        self.train_metrics = metrics.clone('train_')\n",
    "        self.val_metrics = metrics.clone('val_')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        \n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log_dict(self.train_metrics(outputs.view(-1), labels.type(t.int32).view(-1)))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log_dict(self.val_metrics(outputs.view(-1), labels.type(t.int32).view(-1)))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return t.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../config/secret.json')\n",
    "api_key = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "\n",
    "model_checkpoint = pl.callbacks.ModelCheckpoint(dirpath='../src/DiagnosisAI/models/AutoEncoder_UNET/checkpoints')\n",
    "early_stopping = pl.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "logger = pl.loggers.NeptuneLogger(\n",
    "    api_key=api_key['api_neptune'],\n",
    "    project=\"jumpincrane/Binary-slices\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/michalheit/miniconda3/envs/mgr_dp/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1584: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | network       | UNet              | 2.2 M \n",
      "1 | loss_function | BCEWithLogitsLoss | 0     \n",
      "2 | train_metrics | MetricCollection  | 0     \n",
      "3 | val_metrics   | MetricCollection  | 0     \n",
      "----------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.640     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   9%|▊         | 12/141 [01:04<11:36,  5.40s/it, loss=1.42]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michalheit/miniconda3/envs/mgr_dp/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/jumpincrane/Binary-slices/e/BIN-5\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 6 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 6 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/jumpincrane/Binary-slices/e/BIN-5\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(logger=logger, callbacks=[model_checkpoint, early_stopping], gpus=0, max_epochs=100)\n",
    "trainer.fit(segmenter, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "logger.run.stop()\n",
    "t.save(segmenter.state_dict(), \"../src/DiagnosisAI/AutoEncoder_UNET/binary_brats_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best weights\n",
    "segmenter = Segmenter.load_from_checkpoint(model_checkpoint.best_model_path).to(device)  # wczytanie najlepszych wag z treningu\n",
    "segmenter = segmenter.eval()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62b2789806963ee48603215ed199424b464920b314b9e4e70349be42054248a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mgr_dp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e576ea8c3537398da9dcdce81ba2027277556fb4790a3db5b00990a14b94fe39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
